{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AdamW, ElectraConfig, ElectraTokenizer, ElectraForSequenceClassification, ElectraModel, AutoTokenizer, TrainingArguments, DataCollatorWithPadding\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
    "import evaluate\n",
    "from torch.optim import AdamW\n",
    "from finetuning_scheduler import FinetuningScheduler \n",
    "from pytorch_lightning import Trainer, Callback\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelSummary, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics import Accuracy, Precision, Recall\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "import tensorboard\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.optim.lr_scheduler import OneCycleLR, LambdaLR\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./tb_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/workspaces/sarcasm_detection/notebooks/project_data/Sarcasm_Headlines_Dataset_v2.json\"\n",
    "#tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
    "# model = ElectraForSequenceClassification.from_pretrained(\"google/electra-small-discriminator\", num_labels = 2)\n",
    "#configuration = ElectraConfig()\n",
    "#model = ElectraForSequenceClassification(configuration)\n",
    "version_number = 1\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_path = f\"sarcasm_detection_finetune_ckpt_v{version_number}_{current_time}.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data module\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "            self.data = data\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['headline']\n",
    "        labels = self.data[idx]['is_sarcastic']\n",
    "        encodings = self.tokenizer(\n",
    "              text, \n",
    "              max_length=self.max_length, \n",
    "              padding='max_length', \n",
    "              truncation=True, \n",
    "              return_attention_mask=True, \n",
    "              return_token_type_ids=False, \n",
    "              return_tensors='pt'\n",
    "              )\n",
    "        return encodings['input_ids'].flatten(), encodings['attention_mask'].flatten(), torch.tensor(labels)\n",
    "    \n",
    "class SarcasmDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path, batch_size, tokenizer=\"google/electra-small-discriminator\"):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(tokenizer)\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        col_types = {'headline':'str', 'is_sarcastic':'int32'}\n",
    "\n",
    "        df = (\n",
    "             pd.read_json(self.data_path, lines=True)\n",
    "             .drop(columns=['article_link'])       \n",
    "             .astype(col_types)\n",
    "        )\n",
    "\n",
    "        train_df, val_df, test_df = self.split_datasets(df)\n",
    "        # print(f\"training df length: {len(train_df)}\")\n",
    "        # print(f\"Validation DataFrame length: {len(val_df)}\")\n",
    "        # print(f\"Test DataFrame length: {len(test_df)}\")\n",
    "        # print(f\"total df len: {len(train_df+val_df+test_df)}\")\n",
    "        # print(len(df))\n",
    "        \n",
    "        self.data_train = train_df.to_dict('records')\n",
    "        self.data_val = val_df.to_dict('records')\n",
    "        self.data_test = test_df.to_dict('records')\n",
    "        \n",
    "    def setup(self, stage: str=None):\n",
    "\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = SarcasmDataset(self.data_train, self.tokenizer)\n",
    "            self.val_dataset = SarcasmDataset(self.data_val, self.tokenizer)\n",
    "        \n",
    "        if stage == \"test\":\n",
    "            self.test_dataset = SarcasmDataset(self.data_test, self.tokenizer)\n",
    "        \n",
    "        if stage == \"predict\":\n",
    "            self.predict_dataset = SarcasmDataset(self.data_test, self.tokenizer)\n",
    "\n",
    "    def steps_per_epoch(self):\n",
    "        return len(self.train_dataset)\n",
    "    \n",
    "    def split_datasets(self, df):\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=6, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=6, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=6, shuffle=False)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size, num_workers=6, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ElectraClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"google/electra-small-discriminator\", num_labels=2, learning_rate=2e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.warmup_steps = 10000\n",
    "        self.finetuning_scheduler = FinetuningScheduler\n",
    "\n",
    "\n",
    "        #metrics\n",
    "        self.train_accuracy = Accuracy(task='binary', num_classes=num_labels)\n",
    "        self.val_accuracy = Accuracy(task='binary', num_classes=num_labels)\n",
    "        self.train_precision = Precision(task='binary', num_classes=num_labels, average = 'weighted')\n",
    "        self.val_precision = Precision(task='binary', num_classes=num_labels, average = 'weighted')\n",
    "        self.train_recall = Recall(task='binary', num_classes=num_labels, average = 'weighted')\n",
    "        self.val_recall = Recall(task='binary', num_classes=num_labels, average = 'weighted')\n",
    "        self.val_f1_score = BinaryF1Score(task='binary', num_classes=num_labels)\n",
    "\n",
    "        self.finetuning_scheduler.freeze(self.model.electra)\n",
    "\n",
    "        #for adding smaller networks on top\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        #self.fc1 = nn.Linear(self.electra.config.hidden_size, 128)\n",
    "        #self.fc2 = nn.Linear(128, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    \n",
    "        #for adding smaller networks on top\n",
    "        #outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #last_hidden_state = outputs.last_hidden_state\n",
    "        #cls_token = last_hidden_state[:, 0]\n",
    "        #x = self.dropout(cls_token)\n",
    "        #x = self.fc1(x)\n",
    "        #x = torch.relu(x)\n",
    "        #x = self.fc2(x)\n",
    "        #return x\n",
    "    \n",
    "    def on_train_batch_start(self, batch, batch_idx):\n",
    "        if self.global_step == self.warmup_steps:\n",
    "            #unfreeze base layers\n",
    "            for param in self.model.electra.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        #logging\n",
    "        acc = self.train_accuracy(preds, labels)\n",
    "        prec = self.train_precision(preds, labels)\n",
    "        rec = self.train_recall(preds, labels)\n",
    "        self.log(\"train_loss\", loss, on_step = True, on_epoch = True, prog_bar = True)\n",
    "        self.log(\"train_accuracy\", acc, on_step = True, on_epoch = True, prog_bar=True)\n",
    "        self.log(\"train_precision\", prec, on_step = True, on_epoch = True, prog_bar=True)\n",
    "        self.log(\"train_recall\", rec, on_step = True, on_epoch = True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        #logging\n",
    "        acc = self.val_accuracy(preds, labels)\n",
    "        prec = self.val_precision(preds, labels)\n",
    "        rec = self.val_recall(preds, labels)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_accuracy\", acc)\n",
    "        self.log(\"val_precision\", prec)\n",
    "        self.log(\"val_recall\", rec)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        #logging\n",
    "        acc = self.val_accuracy(preds, labels)\n",
    "        prec = self.val_precision(preds, labels)\n",
    "        rec = self.val_recall(preds, labels)\n",
    "        f1 = self.val_f1_score(preds, labels)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        self.log(\"test_accuracy\", acc)\n",
    "        self.log(\"test_precision\", prec)\n",
    "        self.log(\"test_recall\", rec)\n",
    "        self.log(\"test_f1\", f1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        num_epochs = self.trainer.max_epochs\n",
    "\n",
    "        if self.trainer.state.stage == 'fit':\n",
    "            steps_per_epoch = len(self.train_dataloader()) // self.hparams.batch_size\n",
    "        \n",
    "        else:\n",
    "            steps_per_epoch = 1\n",
    "        \n",
    "        total_steps = num_epochs * steps_per_epoch\n",
    "\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = {\n",
    "            'scheduler': OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.hparams.learning_rate,\n",
    "            total_steps=total_steps,\n",
    "            anneal_strategy='linear'\n",
    "            ),\n",
    "            'name':'learning_rate',\n",
    "            'interval':'step',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        return [optimizer],[scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics plotting\n",
    "\n",
    "class MetricsCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_metrics = []\n",
    "        self.val_metrics = []\n",
    "        self.batch_train_metrics = []\n",
    "    \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        self.batch_train_metrics.append(trainer.callback_metrics)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        epoch_train_metrics = {}\n",
    "        for key in self.batch_train_metrics[0].keys():\n",
    "            epoch_train_metrics[key] = torch.stack([x[key] for x in self.batch_train_metrics]).mean()\n",
    "        self.train_metrics.append(epoch_train_metrics)\n",
    "        self.batch_train_metrics = []\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        self.val_metrics.append(trainer.callback_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = ElectraClassifier()\n",
    "data_module = SarcasmDataModule(data_path=data_path, batch_size=16)\n",
    "lr_monitor = LearningRateMonitor(logging_interval = 'step')\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"electra_model_v1\")\n",
    "metrics_callback = MetricsCallback()\n",
    "early_stopping = EarlyStopping('val_loss', patience=10, verbose=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=10000,\n",
    "    callbacks=[lr_monitor, metrics_callback, early_stopping],\n",
    "    logger=logger\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: tb_logs/electra_model_v1\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                             | Params\n",
      "---------------------------------------------------------------------\n",
      "0 | model           | ElectraForSequenceClassification | 13.5 M\n",
      "1 | train_accuracy  | BinaryAccuracy                   | 0     \n",
      "2 | val_accuracy    | BinaryAccuracy                   | 0     \n",
      "3 | train_precision | BinaryPrecision                  | 0     \n",
      "4 | val_precision   | BinaryPrecision                  | 0     \n",
      "5 | train_recall    | BinaryRecall                     | 0     \n",
      "6 | val_recall      | BinaryRecall                     | 0     \n",
      "7 | val_f1_score    | BinaryF1Score                    | 0     \n",
      "---------------------------------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "13.5 M    Non-trainable params\n",
      "13.5 M    Total params\n",
      "54.197    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1431/1431 [01:39<00:00, 14.39it/s, v_num=0, train_loss_step=0.664, train_accuracy_step=0.600, train_precision_step=0.889, train_recall_step=0.615, train_loss_epoch=0.669, train_accuracy_epoch=0.612, train_precision_epoch=0.630, train_recall_epoch=0.311]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1431/1431 [01:38<00:00, 14.51it/s, v_num=0, train_loss_step=0.508, train_accuracy_step=0.800, train_precision_step=0.857, train_recall_step=0.750, train_loss_epoch=0.595, train_accuracy_epoch=0.700, train_precision_epoch=0.726, train_recall_epoch=0.594]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.067 >= min_delta = 0.0. New best score: 0.550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  28%|██▊       | 403/1431 [00:25<01:05, 15.70it/s, v_num=0, train_loss_step=0.420, train_accuracy_step=0.875, train_precision_step=1.000, train_recall_step=0.778, train_loss_epoch=0.595, train_accuracy_epoch=0.700, train_precision_epoch=0.726, train_recall_epoch=0.594] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "trainer.save_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:148: UserWarning: `.predict(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.predict(ckpt_path='best')` to use the best model or `.predict(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at tb_logs/electra_model_v0/version_1/checkpoints/epoch=45-step=65826.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/electra_model_v0/version_1/checkpoints/epoch=45-step=65826.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mpredict(dataloaders\u001b[39m=\u001b[39;49mtest_loader)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:805\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    803\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    804\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 805\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    806\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\n\u001b[1;32m    807\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:847\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, predict_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[1;32m    844\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    845\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    849\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    850\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:973\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_loop\u001b[39m.\u001b[39mrun()\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m--> 973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m    975\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/loops/prediction_loop.py:112\u001b[0m, in \u001b[0;36m_PredictionLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     batch, batch_idx, dataloader_idx \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    113\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/loops/prediction_loop.py:228\u001b[0m, in \u001b[0;36m_PredictionLoop._predict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    227\u001b[0m \u001b[39m# configure step_kwargs\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m predictions \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mpredict_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    230\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m predictions \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 288\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    290\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    291\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:396\u001b[0m, in \u001b[0;36mStrategy.predict_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpredict_step_context():\n\u001b[1;32m    395\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, PredictStep)\n\u001b[0;32m--> 396\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/core/module.py:887\u001b[0m, in \u001b[0;36mLightningModule.predict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_step\u001b[39m(\u001b[39mself\u001b[39m, batch: Any, batch_idx: \u001b[39mint\u001b[39m, dataloader_idx: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    853\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Step function called during :meth:`~pytorch_lightning.trainer.trainer.Trainer.predict`. By default, it\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[39m    calls :meth:`~pytorch_lightning.core.module.LightningModule.forward`. Override to add any processing logic.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39m        Predicted output\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(batch)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'attention_mask'"
     ]
    }
   ],
   "source": [
    "trainer.predict(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b59c660d1e521bda\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b59c660d1e521bda\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=tb_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m val_accuracies \u001b[39m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m train_metric, val_metric \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(metrics_callback\u001b[39m.\u001b[39mtrain_metrics, metrics_callback\u001b[39m.\u001b[39mval_metrics):\n\u001b[0;32m----> 9\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_metric[\u001b[39m\"\u001b[39;49m\u001b[39mtrain_loss\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     10\u001b[0m     train_accuracy\u001b[39m.\u001b[39mappend(train_metric[\u001b[39m\"\u001b[39m\u001b[39mtrain_accuracy\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     11\u001b[0m     val_losses\u001b[39m.\u001b[39mappend(val_metric[\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train_loss'"
     ]
    }
   ],
   "source": [
    "#plotting metrics\n",
    "\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for train_metric, val_metric in zip(metrics_callback.train_metrics, metrics_callback.val_metrics):\n",
    "    train_losses.append(train_metric[\"train_loss\"].item())\n",
    "    train_accuracy.append(train_metric[\"train_accuracy\"].item())\n",
    "    val_losses.append(val_metric[\"val_loss\"].item())\n",
    "    val_accuracies.append(val_metric[\"val_accuracy\"].item())\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"train_loss\")\n",
    "plt.plot(val_losses, label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(train_accuracy, label='training_accuracy')\n",
    "plt.plot(val_accuracies, label=\"val_accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find most recent version of model checkpoint\n",
    "\n",
    "def find_latest_checkpoint(version_prefix = \"sarcasm_detection_finetune_ckpt_v\"):\n",
    "    checkpoints = [file for file in os.listdir() if file.startswith(version_prefix) and file.endswith(\".ckpt\")]\n",
    "    return max(checkpoints, key=os.path.getctime) if checkpoints else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ElectraClassifier' object has no attribute 'electra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     transfer_model \u001b[39m=\u001b[39m ElectraClassifier(num_labels\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[39m#load fine_tuned model weights keeping new classification head\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     transfer_model\u001b[39m.\u001b[39;49melectra\u001b[39m.\u001b[39mload_state_dict(loaded_model\u001b[39m.\u001b[39melectra\u001b[39m.\u001b[39mstate_dict())\n\u001b[1;32m     14\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNo model checkpoint found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ElectraClassifier' object has no attribute 'electra'"
     ]
    }
   ],
   "source": [
    "latest_checkpoint = find_latest_checkpoint()\n",
    "\n",
    "if latest_checkpoint:\n",
    "\n",
    "    #load model\n",
    "    loaded_model = ElectraClassifier.load_from_checkpoint(latest_checkpoint)\n",
    "\n",
    "    #new model with different classification heads\n",
    "    transfer_model = ElectraClassifier(num_labels=6)\n",
    "\n",
    "    #load fine_tuned model weights keeping new classification head\n",
    "    transfer_model.electra.load_state_dict(loaded_model.electra.state_dict())\n",
    "\n",
    "else:\n",
    "    print(\"No model checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 179it [00:09, 18.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_accuracy       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9109326004981995     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8960329294204712     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2756422758102417     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_precision       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9579657912254333     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_recall        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8511241674423218     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_accuracy      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9109326004981995    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8960329294204712    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2756422758102417    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_precision      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9579657912254333    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m       test_recall       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8511241674423218    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraClassifier(\n",
       "  (model): ElectraForSequenceClassification(\n",
       "    (electra): ElectraModel(\n",
       "      (embeddings): ElectraEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 128)\n",
       "        (token_type_embeddings): Embedding(2, 128)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (encoder): ElectraEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): ElectraClassificationHead(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=256, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (train_accuracy): BinaryAccuracy()\n",
       "  (val_accuracy): BinaryAccuracy()\n",
       "  (train_precision): BinaryPrecision()\n",
       "  (val_precision): BinaryPrecision()\n",
       "  (train_recall): BinaryRecall()\n",
       "  (val_recall): BinaryRecall()\n",
       "  (val_f1_score): BinaryF1Score()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with a different number of classes\n",
    "transfer_model = ElectraClassifier(num_labels=3)\n",
    "\n",
    "# Load the fine-tuned model weights, but keep the new classification head\n",
    "transfer_model.model.electra.load_state_dict(loaded_model.model.electra.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='.',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\", \"accuracy\", \"precision\")\n",
    "results = metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"accuracy\", \"precision\", \"f1\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions = predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\", \"accuracy\", \"precision\")\n",
    "results = metric.compute(predictions=preds, references=predictions.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_encodings = tokenizer.batch_encode_plus(list(train_df['headline']), max_length=512, padding='max_length', truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='pt')\n",
    "# val_encodings = tokenizer.batch_encode_plus(list(val_df['headline']), max_length=512, padding='max_length', truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='pt')\n",
    "# test_encodings = tokenizer.batch_encode_plus(list(test_df['headline']), max_length=512, padding=True, truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='pt')\n",
    "\n",
    "# train_input_ids = torch.tensor(train_encodings['input_ids'])\n",
    "# train_attention_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "# val_input_ids = torch.tensor(val_encodings['input_ids'])\n",
    "# val_attention_masks = torch.tensor(val_encodings['attention_mask'])\n",
    "# test_input_ids = torch.tensor(test_encodings['input_ids'])\n",
    "# test_attention_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "\n",
    "# train_labels = torch.tensor(train_df['is_sarcastic'].values)\n",
    "# val_labels = torch.tensor(val_df['is_sarcastic'].values)\n",
    "# test_labels = torch.tensor(test_df['is_sarcastic'].values)\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
