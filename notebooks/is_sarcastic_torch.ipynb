{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AdamW, ElectraConfig, ElectraTokenizer, ElectraForSequenceClassification, ElectraModel, AutoTokenizer, TrainingArguments, DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
    "import evaluate\n",
    "from torch.optim import AdamW\n",
    "from finetuning_scheduler import FinetuningScheduler \n",
    "from pytorch_lightning import Trainer, Callback\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelSummary, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchmetrics import Accuracy, Precision, Recall\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "import tensorboard\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.optim.lr_scheduler import OneCycleLR, LambdaLR\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./tb_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/workspaces/sarcasm_detection/notebooks/project_data/Sarcasm_Headlines_Dataset_v2.json\"\n",
    "#tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
    "# model = ElectraForSequenceClassification.from_pretrained(\"google/electra-small-discriminator\", num_labels = 2)\n",
    "#configuration = ElectraConfig()\n",
    "#model = ElectraForSequenceClassification(configuration)\n",
    "version_number = 1\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_path = f\"sarcasm_detection_finetune_ckpt_v{version_number}_{current_time}.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data module\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "            self.data = data\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['headline']\n",
    "        labels = self.data[idx]['is_sarcastic']\n",
    "        encodings = self.tokenizer(\n",
    "              text, \n",
    "              max_length=self.max_length, \n",
    "              padding='max_length', \n",
    "              truncation=True, \n",
    "              return_attention_mask=True, \n",
    "              return_token_type_ids=False, \n",
    "              return_tensors='pt'\n",
    "              )\n",
    "        return encodings['input_ids'].flatten(), encodings['attention_mask'].flatten(), torch.tensor(labels)\n",
    "    \n",
    "class SarcasmDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path, batch_size, tokenizer=\"google/electra-small-discriminator\"):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(tokenizer)\n",
    "        self.batch_size=batch_size\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        col_types = {'headline':'str', 'is_sarcastic':'int32'}\n",
    "\n",
    "        df = (\n",
    "             pd.read_json(self.data_path, lines=True)\n",
    "             .drop(columns=['article_link'])       \n",
    "             .astype(col_types)\n",
    "        )\n",
    "\n",
    "        train_df, val_df, test_df = self.split_datasets(df)\n",
    "        # print(f\"training df length: {len(train_df)}\")\n",
    "        # print(f\"Validation DataFrame length: {len(val_df)}\")\n",
    "        # print(f\"Test DataFrame length: {len(test_df)}\")\n",
    "        # print(f\"total df len: {len(train_df+val_df+test_df)}\")\n",
    "        # print(len(df))\n",
    "        \n",
    "        self.data_train = train_df.to_dict('records')\n",
    "        self.data_val = val_df.to_dict('records')\n",
    "        self.data_test = test_df.to_dict('records')\n",
    "        \n",
    "    def setup(self, stage: str=None):\n",
    "\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = SarcasmDataset(self.data_train, self.tokenizer)\n",
    "            self.val_dataset = SarcasmDataset(self.data_val, self.tokenizer)\n",
    "        \n",
    "        if stage == \"test\":\n",
    "            self.test_dataset = SarcasmDataset(self.data_test, self.tokenizer)\n",
    "        \n",
    "        if stage == \"predict\":\n",
    "            self.predict_dataset = SarcasmDataset(self.data_test, self.tokenizer)\n",
    "\n",
    "    def steps_per_epoch(self):\n",
    "        return len(self.train_dataset)\n",
    "    \n",
    "    def split_datasets(self, df):\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=6, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=6, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=6, shuffle=False)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size, num_workers=6, shuffle=False, collate_fn=self.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ElectraClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"google/electra-small-discriminator\", num_labels=2, learning_rate=2e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.warmup_steps = 10000\n",
    "        self.finetuning_scheduler = FinetuningScheduler\n",
    "\n",
    "\n",
    "        #metrics\n",
    "        self.train_accuracy = Accuracy(task='binary', num_classes=num_labels)\n",
    "        self.val_accuracy = Accuracy(task='binary', num_classes=num_labels)\n",
    "        self.train_precision = Precision(task='binary', num_classes=num_labels, average = 'weighted')\n",
    "        self.val_precision = Precision(task='binary', num_classes=num_labels, average = 'weighted')\n",
    "        self.train_recall = Recall(task='binary', num_classes=num_labels, average = 'weighted')\n",
    "        self.val_recall = Recall(task='binary', num_classes=num_labels, average = 'weighted')\n",
    "        self.val_f1_score = BinaryF1Score(task='binary', num_classes=num_labels)\n",
    "        self.f1 = BinaryF1Score(task='binary', num_classes=num_labels, average = 'macro')\n",
    "\n",
    "        self.finetuning_scheduler.freeze(self.model.electra)\n",
    "\n",
    "        #for adding smaller networks on top\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        #self.fc1 = nn.Linear(self.electra.config.hidden_size, 128)\n",
    "        #self.fc2 = nn.Linear(128, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    \n",
    "        #for adding smaller networks on top\n",
    "        #outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #last_hidden_state = outputs.last_hidden_state\n",
    "        #cls_token = last_hidden_state[:, 0]\n",
    "        #x = self.dropout(cls_token)\n",
    "        #x = self.fc1(x)\n",
    "        #x = torch.relu(x)\n",
    "        #x = self.fc2(x)\n",
    "        #return x\n",
    "    \n",
    "    def on_train_batch_start(self, batch, batch_idx):\n",
    "        if self.global_step == self.warmup_steps:\n",
    "            #unfreeze base layers\n",
    "            for param in self.model.electra.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        #logging\n",
    "        acc = self.train_accuracy(preds, labels)\n",
    "        prec = self.train_precision(preds, labels)\n",
    "        rec = self.train_recall(preds, labels)\n",
    "        self.log(\"train_loss\", loss, on_step = True, on_epoch = True, prog_bar = True)\n",
    "        self.log(\"train_accuracy\", acc, on_step = True, on_epoch = True, prog_bar=True)\n",
    "        self.log(\"train_precision\", prec, on_step = True, on_epoch = True, prog_bar=True)\n",
    "        self.log(\"train_recall\", rec, on_step = True, on_epoch = True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        #logging\n",
    "        acc = self.val_accuracy(preds, labels)\n",
    "        prec = self.val_precision(preds, labels)\n",
    "        rec = self.val_recall(preds, labels)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_accuracy\", acc)\n",
    "        self.log(\"val_precision\", prec)\n",
    "        self.log(\"val_recall\", rec)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        f1_score = self.f1(preds, labels)\n",
    "        self.log('test_f1', f1_score, on_step=True, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        return preds\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        inputs, labels = zip(*batch)\n",
    "        return torch.stack(inputs), torch.stack(labels)\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self.optimizers().param_groups[0]['lr']\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # num_epochs = self.trainer.max_epochs\n",
    "\n",
    "        # if self.trainer.state.stage == 'fit':\n",
    "        #     steps_per_epoch = len(self.train_dataloader()) // self.hparams.batch_size\n",
    "        \n",
    "        # else:\n",
    "        #     steps_per_epoch = 1\n",
    "\n",
    "        # warmup_steps = 10000\n",
    "        \n",
    "        # total_steps = num_epochs * steps_per_epoch\n",
    "\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler_config = {\n",
    "        'scheduler': FinetuningScheduler(\n",
    "            optimizer,\n",
    "            milestones=[\n",
    "                (0, 0.00002),\n",
    "                (10000, 0.0002),\n",
    "                (20000, 0.002)\n",
    "            ],\n",
    "        ),\n",
    "        'name': 'learning_rate',\n",
    "        'interval': 'step',\n",
    "        'frequency': 1\n",
    "    }\n",
    "        return [optimizer],[scheduler_config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics plotting\n",
    "\n",
    "class MetricsCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_metrics = []\n",
    "        self.val_metrics = []\n",
    "        self.batch_train_metrics = []\n",
    "    \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        self.batch_train_metrics.append(trainer.callback_metrics)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        epoch_train_metrics = {}\n",
    "        for key in self.batch_train_metrics[0].keys():\n",
    "            epoch_train_metrics[key] = torch.stack([x[key] for x in self.batch_train_metrics]).mean()\n",
    "        self.train_metrics.append(epoch_train_metrics)\n",
    "        self.batch_train_metrics = []\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        self.val_metrics.append(trainer.callback_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = ElectraClassifier()\n",
    "data_module = SarcasmDataModule(data_path=data_path, batch_size=16)\n",
    "lr_monitor = LearningRateMonitor(logging_interval = 'step')\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"electra_model_v1\")\n",
    "metrics_callback = MetricsCallback()\n",
    "early_stopping = EarlyStopping('val_loss', patience=10, verbose=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=10000,\n",
    "    callbacks=[lr_monitor, metrics_callback, early_stopping],\n",
    "    logger=logger\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: tb_logs/electra_model_v1\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'milestones'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, data_module)\n\u001b[1;32m      3\u001b[0m trainer\u001b[39m.\u001b[39msave_checkpoint(checkpoint_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:520\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    518\u001b[0m model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    519\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 520\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    521\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    550\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    551\u001b[0m )\n\u001b[1;32m    553\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    554\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    555\u001b[0m     ckpt_path,\n\u001b[1;32m    556\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    557\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    558\u001b[0m )\n\u001b[0;32m--> 559\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    561\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    562\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:911\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mreset_metrics()\n\u001b[1;32m    910\u001b[0m \u001b[39m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49msetup(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    913\u001b[0m \u001b[39m# hook\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39m==\u001b[39m TrainerFn\u001b[39m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/strategies/single_device.py:74\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup\u001b[39m(\u001b[39mself\u001b[39m, trainer: pl\u001b[39m.\u001b[39mTrainer) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_to_device()\n\u001b[0;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msetup(trainer)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:148\u001b[0m, in \u001b[0;36mStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39msetup(trainer)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_optimizers(trainer)\n\u001b[1;32m    149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetup_precision_plugin()\n\u001b[1;32m    150\u001b[0m _optimizers_to_device(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:138\u001b[0m, in \u001b[0;36mStrategy.setup_optimizers\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler_configs \u001b[39m=\u001b[39m _init_optimizers_and_lr_schedulers(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py:171\u001b[0m, in \u001b[0;36m_init_optimizers_and_lr_schedulers\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m call\n\u001b[0;32m--> 171\u001b[0m optim_conf \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(model\u001b[39m.\u001b[39;49mtrainer, \u001b[39m\"\u001b[39;49m\u001b[39mconfigure_optimizers\u001b[39;49m\u001b[39m\"\u001b[39;49m, pl_module\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m optim_conf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     rank_zero_warn(\n\u001b[1;32m    175\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    176\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:142\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    141\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    144\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    145\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[0;32mIn[5], line 114\u001b[0m, in \u001b[0;36mElectraClassifier.configure_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfigure_optimizers\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    100\u001b[0m     \u001b[39m# num_epochs = self.trainer.max_epochs\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \n\u001b[1;32m    110\u001b[0m     \u001b[39m# total_steps = num_epochs * steps_per_epoch\u001b[39;00m\n\u001b[1;32m    112\u001b[0m     optimizer \u001b[39m=\u001b[39m AdamW(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mlearning_rate)\n\u001b[1;32m    113\u001b[0m     scheduler_config \u001b[39m=\u001b[39m {\n\u001b[0;32m--> 114\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mscheduler\u001b[39m\u001b[39m'\u001b[39m: FinetuningScheduler(\n\u001b[1;32m    115\u001b[0m         optimizer,\n\u001b[1;32m    116\u001b[0m         milestones\u001b[39m=\u001b[39;49m[\n\u001b[1;32m    117\u001b[0m             (\u001b[39m0\u001b[39;49m, \u001b[39m0.00002\u001b[39;49m),\n\u001b[1;32m    118\u001b[0m             (\u001b[39m10000\u001b[39;49m, \u001b[39m0.0002\u001b[39;49m),\n\u001b[1;32m    119\u001b[0m             (\u001b[39m20000\u001b[39;49m, \u001b[39m0.002\u001b[39;49m)\n\u001b[1;32m    120\u001b[0m         ],\n\u001b[1;32m    121\u001b[0m     ),\n\u001b[1;32m    122\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    123\u001b[0m     \u001b[39m'\u001b[39m\u001b[39minterval\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    124\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfrequency\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m\n\u001b[1;32m    125\u001b[0m }\n\u001b[1;32m    126\u001b[0m     \u001b[39mreturn\u001b[39;00m [optimizer],[scheduler_config]\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'milestones'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "trainer.save_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6a4a760087a4201a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6a4a760087a4201a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=tb_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:148: UserWarning: `.predict(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.predict(ckpt_path='best')` to use the best model or `.predict(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at tb_logs/electra_model_v0/version_1/checkpoints/epoch=45-step=65826.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at tb_logs/electra_model_v0/version_1/checkpoints/epoch=45-step=65826.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 0/179 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mpredict(dataloaders\u001b[39m=\u001b[39;49mtest_loader)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:805\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    803\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    804\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 805\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    806\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\n\u001b[1;32m    807\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:847\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, predict_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[1;32m    844\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    845\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    849\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    850\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:973\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_loop\u001b[39m.\u001b[39mrun()\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m--> 973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m    975\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/loops/prediction_loop.py:112\u001b[0m, in \u001b[0;36m_PredictionLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     batch, batch_idx, dataloader_idx \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(data_fetcher)\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[0;32m--> 112\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    113\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/loops/prediction_loop.py:228\u001b[0m, in \u001b[0;36m_PredictionLoop._predict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    227\u001b[0m \u001b[39m# configure step_kwargs\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m predictions \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mpredict_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    230\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m predictions \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 288\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    290\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    291\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:396\u001b[0m, in \u001b[0;36mStrategy.predict_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpredict_step_context():\n\u001b[1;32m    395\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, PredictStep)\n\u001b[0;32m--> 396\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/core/module.py:887\u001b[0m, in \u001b[0;36mLightningModule.predict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_step\u001b[39m(\u001b[39mself\u001b[39m, batch: Any, batch_idx: \u001b[39mint\u001b[39m, dataloader_idx: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    853\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Step function called during :meth:`~pytorch_lightning.trainer.trainer.Trainer.predict`. By default, it\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[39m    calls :meth:`~pytorch_lightning.core.module.LightningModule.forward`. Override to add any processing logic.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39m        Predicted output\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(batch)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'attention_mask'"
     ]
    }
   ],
   "source": [
    "trainer.predict(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find most recent version of model checkpoint\n",
    "\n",
    "def find_latest_checkpoint(version_prefix = \"sarcasm_detection_finetune_ckpt_v\"):\n",
    "    checkpoints = [file for file in os.listdir() if file.startswith(version_prefix) and file.endswith(\".ckpt\")]\n",
    "    return max(checkpoints, key=os.path.getctime) if checkpoints else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ElectraClassifier' object has no attribute 'electra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     transfer_model \u001b[39m=\u001b[39m ElectraClassifier(num_labels\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[39m#load fine_tuned model weights keeping new classification head\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     transfer_model\u001b[39m.\u001b[39;49melectra\u001b[39m.\u001b[39mload_state_dict(loaded_model\u001b[39m.\u001b[39melectra\u001b[39m.\u001b[39mstate_dict())\n\u001b[1;32m     14\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNo model checkpoint found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ElectraClassifier' object has no attribute 'electra'"
     ]
    }
   ],
   "source": [
    "latest_checkpoint = find_latest_checkpoint()\n",
    "\n",
    "if latest_checkpoint:\n",
    "\n",
    "    #load model\n",
    "    loaded_model = ElectraClassifier.load_from_checkpoint(latest_checkpoint)\n",
    "\n",
    "    #new model with different classification heads\n",
    "    transfer_model = ElectraClassifier(num_labels=6)\n",
    "\n",
    "    #load fine_tuned model weights keeping new classification head\n",
    "    transfer_model.electra.load_state_dict(loaded_model.electra.state_dict())\n",
    "\n",
    "else:\n",
    "    print(\"No model checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraClassifier(\n",
       "  (model): ElectraForSequenceClassification(\n",
       "    (electra): ElectraModel(\n",
       "      (embeddings): ElectraEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 128)\n",
       "        (token_type_embeddings): Embedding(2, 128)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (encoder): ElectraEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): ElectraClassificationHead(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=256, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (train_accuracy): BinaryAccuracy()\n",
       "  (val_accuracy): BinaryAccuracy()\n",
       "  (train_precision): BinaryPrecision()\n",
       "  (val_precision): BinaryPrecision()\n",
       "  (train_recall): BinaryRecall()\n",
       "  (val_recall): BinaryRecall()\n",
       "  (val_f1_score): BinaryF1Score()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with a different number of classes\n",
    "transfer_model = ElectraClassifier(num_labels=3)\n",
    "\n",
    "# Load the fine-tuned model weights, but keep the new classification head\n",
    "transfer_model.model.electra.load_state_dict(loaded_model.model.electra.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='.',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\", \"accuracy\", \"precision\")\n",
    "results = metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"accuracy\", \"precision\", \"f1\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions = predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\", \"accuracy\", \"precision\")\n",
    "results = metric.compute(predictions=preds, references=predictions.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_encodings = tokenizer.batch_encode_plus(list(train_df['headline']), max_length=512, padding='max_length', truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='pt')\n",
    "# val_encodings = tokenizer.batch_encode_plus(list(val_df['headline']), max_length=512, padding='max_length', truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='pt')\n",
    "# test_encodings = tokenizer.batch_encode_plus(list(test_df['headline']), max_length=512, padding=True, truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='pt')\n",
    "\n",
    "# train_input_ids = torch.tensor(train_encodings['input_ids'])\n",
    "# train_attention_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "# val_input_ids = torch.tensor(val_encodings['input_ids'])\n",
    "# val_attention_masks = torch.tensor(val_encodings['attention_mask'])\n",
    "# test_input_ids = torch.tensor(test_encodings['input_ids'])\n",
    "# test_attention_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "\n",
    "# train_labels = torch.tensor(train_df['is_sarcastic'].values)\n",
    "# val_labels = torch.tensor(val_df['is_sarcastic'].values)\n",
    "# test_labels = torch.tensor(test_df['is_sarcastic'].values)\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
