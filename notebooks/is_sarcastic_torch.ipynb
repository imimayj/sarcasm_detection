{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AdamW, ElectraConfig, ElectraTokenizer, ElectraForSequenceClassification, ElectraModel, AutoTokenizer, TrainingArguments, DataCollatorWithPadding, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
    "import evaluate\n",
    "from torch.optim import AdamW\n",
    "from finetuning_scheduler import FinetuningScheduler\n",
    "from pytorch_lightning import Trainer, Callback\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelSummary, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torchmetrics\n",
    "from torchmetrics import Accuracy, Precision, Recall\n",
    "from torchmetrics.classification import BinaryF1Score\n",
    "import tensorboard\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.optim.lr_scheduler import OneCycleLR, LambdaLR, CosineAnnealingLR\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./tb_logs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/workspaces/sarcasm_detection/notebooks/project_data/Sarcasm_Headlines_Dataset_v2.json\"\n",
    "#tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
    "# model = ElectraForSequenceClassification.from_pretrained(\"google/electra-small-discriminator\", num_labels = 2)\n",
    "#configuration = ElectraConfig()\n",
    "#model = ElectraForSequenceClassification(configuration)\n",
    "version_number = 1\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_path = f\"sarcasm_detection_finetune_ckpt_v{version_number}_{current_time}.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data module\n",
    "class SarcasmDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "            self.data = data\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['headline']\n",
    "        labels = self.data[idx]['is_sarcastic']\n",
    "        encodings = self.tokenizer(\n",
    "              text, \n",
    "              max_length=self.max_length, \n",
    "              padding='max_length', \n",
    "              truncation=True, \n",
    "              return_attention_mask=True, \n",
    "              return_token_type_ids=False, \n",
    "              return_tensors='pt'\n",
    "              )\n",
    "        return encodings['input_ids'].flatten(), encodings['attention_mask'].flatten(), torch.tensor(labels)\n",
    "    \n",
    "class SarcasmDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path, batch_size, tokenizer=\"google/electra-small-discriminator\"):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(tokenizer)\n",
    "        self.batch_size=batch_size\n",
    "        self.collate_fn = self.default_collate_fn\n",
    "\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        col_types = {'headline':'str', 'is_sarcastic':'int32'}\n",
    "\n",
    "        df = (\n",
    "             pd.read_json(self.data_path, lines=True)\n",
    "             .drop(columns=['article_link'])       \n",
    "             .astype(col_types)\n",
    "        )\n",
    "\n",
    "        train_df, val_df, test_df = self.split_datasets(df)\n",
    "        # print(f\"training df length: {len(train_df)}\")\n",
    "        # print(f\"Validation DataFrame length: {len(val_df)}\")\n",
    "        # print(f\"Test DataFrame length: {len(test_df)}\")\n",
    "        # print(f\"total df len: {len(train_df+val_df+test_df)}\")\n",
    "        # print(len(df))\n",
    "        \n",
    "        self.data_train = train_df.to_dict('records')\n",
    "        self.data_val = val_df.to_dict('records')\n",
    "        self.data_test = test_df.to_dict('records')\n",
    "        \n",
    "    def setup(self, stage: str=None):\n",
    "\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = SarcasmDataset(self.data_train, self.tokenizer)\n",
    "            self.val_dataset = SarcasmDataset(self.data_val, self.tokenizer)\n",
    "        \n",
    "        if stage == \"test\":\n",
    "            self.test_dataset = SarcasmDataset(self.data_test, self.tokenizer)\n",
    "        \n",
    "        if stage == \"predict\":\n",
    "            self.predict_dataset = SarcasmDataset(self.data_test, self.tokenizer)\n",
    "\n",
    "    def steps_per_epoch(self):\n",
    "        return len(self.train_dataset)\n",
    "    \n",
    "    def split_datasets(self, df):\n",
    "        train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=6, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=6, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=6, shuffle=False)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_dataset, batch_size=self.batch_size, num_workers=6, shuffle=False, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def default_collate_fn(self, batch):\n",
    "        input_ids, attention_mask, labels = zip(*batch)\n",
    "        input_ids = torch.stack(input_ids)\n",
    "        attention_mask = torch.stack(attention_mask)\n",
    "        labels = torch.stack(labels)\n",
    "        return input_ids, attention_mask, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ElectraClassifier(pl.LightningModule):\n",
    "    def __init__(self, data_module, batch_size, model_name=\"google/electra-small-discriminator\", num_labels=2, learning_rate=2e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = ElectraForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.data_module = data_module\n",
    "        self.num_layers = len(list(self.parameters()))\n",
    "        self.warmup_steps = 10000\n",
    "        #self.finetuning_scheduler = FinetuningScheduler()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.predictions = []\n",
    "        self.targets = []\n",
    "\n",
    "        for param in self.model.electra.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        #metrics\n",
    "        self.train_accuracy = Accuracy(task='binary', num_classes=num_labels)\n",
    "        self.val_accuracy = Accuracy(task='binary', num_classes=num_labels)\n",
    "        self.train_precision = Precision(task='binary', num_classes=num_labels, average = 'weighted')\n",
    "        self.val_precision = Precision(task='binary', num_classes=num_labels, average = 'weighted')\n",
    "        self.train_recall = Recall(task='binary', num_classes=num_labels, average = 'weighted')\n",
    "        self.val_recall = Recall(task='binary', num_classes=num_labels, average = 'weighted')\n",
    "        self.val_f1_score = BinaryF1Score(task='binary', num_classes=num_labels)\n",
    "        self.f1 = BinaryF1Score(task='binary', num_classes=num_labels, average = 'macro')\n",
    "\n",
    "        #self.finetuning_scheduler.freeze(self.model.electra)\n",
    "\n",
    "        #for adding smaller networks on top\n",
    "        #self.dropout = nn.Dropout(0.1)\n",
    "        #self.fc1 = nn.Linear(self.electra.config.hidden_size, 128)\n",
    "        #self.fc2 = nn.Linear(128, num_labels)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.data_module.train_dataloader()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        return self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    \n",
    "        #for adding smaller networks on top\n",
    "        #outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #last_hidden_state = outputs.last_hidden_state\n",
    "        #cls_token = last_hidden_state[:, 0]\n",
    "        #x = self.dropout(cls_token)\n",
    "        #x = self.fc1(x)\n",
    "        #x = torch.relu(x)\n",
    "        #x = self.fc2(x)\n",
    "        #return x\n",
    "    \n",
    "    def on_train_start(self):\n",
    "        self.train_dataset_len = len(self.train_dataloader().dataset)\n",
    "    \n",
    "    def on_train_batch_start(self, batch, batch_idx):\n",
    "\n",
    "        if self.global_step == self.warmup_steps:\n",
    "            #unfreeze base layers\n",
    "            print(\"unfrozen\")\n",
    "            for param in self.model.electra.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "            steps_per_epoch = len(self.trainer.datamodule.train_dataloader()) // self.hparams.batch_size\n",
    "\n",
    "        \n",
    "        #print(\"Current learning rate at global step {}: {}\".format(self.global_step, self.current_lr))\n",
    "\n",
    "    def on_test_epoch_end(self, test_step_outputs):\n",
    "        all_preds = torch.cat([x['preds'] for x in test_step_outputs])\n",
    "        all_labels = torch.cat([x['labels'] for x in test_step_outputs])\n",
    "        return {'preds': all_preds, 'labels': all_labels}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        #logging\n",
    "        acc = self.train_accuracy(preds, labels)\n",
    "        prec = self.train_precision(preds, labels)\n",
    "        rec = self.train_recall(preds, labels)\n",
    "        self.log(\"train_loss\", loss, on_step = True, on_epoch = True, prog_bar = True)\n",
    "        self.log(\"train_accuracy\", acc, on_step = True, on_epoch = True, prog_bar=True)\n",
    "        self.log(\"train_precision\", prec, on_step = True, on_epoch = True, prog_bar=True)\n",
    "        self.log(\"train_recall\", rec, on_step = True, on_epoch = True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        #logging\n",
    "        acc = self.val_accuracy(preds, labels)\n",
    "        prec = self.val_precision(preds, labels)\n",
    "        rec = self.val_recall(preds, labels)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_accuracy\", acc)\n",
    "        self.log(\"val_precision\", prec)\n",
    "        self.log(\"val_recall\", rec)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        f1_score = self.f1(preds, labels)\n",
    "        self.log('test_f1', f1_score, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return {'preds': preds, 'labels': labels}\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = self(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "        return {'preds':preds, 'labels':labels}\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        inputs, labels = zip(*batch)\n",
    "        return torch.stack(inputs), torch.stack(labels)\n",
    "\n",
    "    @property\n",
    "    def current_lr(self):\n",
    "        return self.optimizers().param_groups[0]['lr']\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "    \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "\n",
    "        scheduler = {\n",
    "            'scheduler': scheduler,\n",
    "            'interval': 'step',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    # def configure_optimizers(self):\n",
    "\n",
    "    #     num_epochs = self.trainer.max_epochs\n",
    "\n",
    "    #     if self.trainer.state.stage == 'fit':\n",
    "    #         #steps_per_epoch = len(self.train_dataloader()) // self.hparams.batch_size\n",
    "    #         steps_per_epoch = len(self.trainer.datamodule.train_dataloader())\n",
    "    #     else:\n",
    "    #         steps_per_epoch = 1\n",
    "        \n",
    "    #     total_steps = num_epochs * steps_per_epoch\n",
    "\n",
    "    #     optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "    #     scheduler = {\n",
    "    #         'scheduler': OneCycleLR(\n",
    "    #             optimizer,\n",
    "    #             max_lr=self.hparams.learning_rate,\n",
    "    #             total_steps=total_steps,\n",
    "    #             anneal_strategy='linear'\n",
    "    #         ),\n",
    "    #         'name':'learning_rate',\n",
    "    #         'interval':'step',\n",
    "    #         'frequency': 1\n",
    "    #     }\n",
    "    #     return [optimizer],[scheduler]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElectraSubClassification(pl.LightningModule):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics plotting\n",
    "\n",
    "class MetricsCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_metrics = []\n",
    "        self.val_metrics = []\n",
    "        self.batch_train_metrics = []\n",
    "    \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        self.batch_train_metrics.append(trainer.callback_metrics)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        epoch_train_metrics = {}\n",
    "        for key in self.batch_train_metrics[0].keys():\n",
    "            epoch_train_metrics[key] = torch.stack([x[key] for x in self.batch_train_metrics]).mean()\n",
    "        self.train_metrics.append(epoch_train_metrics)\n",
    "        self.batch_train_metrics = []\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        self.val_metrics.append(trainer.callback_metrics)\n",
    "\n",
    "# class CustomFineTuningCallback(Callback):\n",
    "#         def __init__(self, num_layers, num_layers_to_unfreeze, warmup_steps):\n",
    "#              self.num_layers = num_layers\n",
    "#              self.num_layers_to_unfreeze = num_layers_to_unfreeze\n",
    "#              self.warmup_steps = warmup_steps\n",
    "#              self.optimizer = None\n",
    "        \n",
    "#         def on_train_start(self, trainer, pl_module):\n",
    "#             self.optimizer = trainer.optimizers[0]\n",
    "\n",
    "\n",
    "#         def on_train_batch_start(self, trainer, pl_module, batch, batch_idx):\n",
    "#             global_step = trainer.global_step\n",
    "\n",
    "#             if global_step >= self.warmup_steps:\n",
    "#                   steps_since_warmup = global_step - self.warmup_steps\n",
    "\n",
    "#                   if steps_since_warmup % self.num_layers_to_unfreeze == 0:\n",
    "#                     layer_to_unfreeze = self.num_layers - (steps_since_warmup // self.num_layers_to_unfreeze)\n",
    "#                   for i, param_group in enumerate(trainer.optimizers[0].param_groups):\n",
    "#                     param_group[\"requires_grad\"] = i >= layer_to_unfreeze\n",
    "            \n",
    "#             trainer.callbacks[1].on_train_batch_start(trainer, pl_module, batch, batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "data_module = SarcasmDataModule(data_path=data_path, batch_size=16)\n",
    "model = ElectraClassifier(data_module = data_module, batch_size=data_module.batch_size)\n",
    "lr_monitor = LearningRateMonitor(logging_interval = 'step')\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"electra_model_v1\")\n",
    "metrics_callback = MetricsCallback()\n",
    "early_stopping = EarlyStopping('val_loss', patience=5, verbose=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=10000,\n",
    "    callbacks=[\n",
    "        lr_monitor, \n",
    "        metrics_callback, \n",
    "        early_stopping,\n",
    "    ],\n",
    "    logger=logger\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: tb_logs/electra_model_v1\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name            | Type                             | Params\n",
      "---------------------------------------------------------------------\n",
      "0 | model           | ElectraForSequenceClassification | 13.5 M\n",
      "1 | train_accuracy  | BinaryAccuracy                   | 0     \n",
      "2 | val_accuracy    | BinaryAccuracy                   | 0     \n",
      "3 | train_precision | BinaryPrecision                  | 0     \n",
      "4 | val_precision   | BinaryPrecision                  | 0     \n",
      "5 | train_recall    | BinaryRecall                     | 0     \n",
      "6 | val_recall      | BinaryRecall                     | 0     \n",
      "7 | val_f1_score    | BinaryF1Score                    | 0     \n",
      "8 | f1              | BinaryF1Score                    | 0     \n",
      "---------------------------------------------------------------------\n",
      "66.3 K    Trainable params\n",
      "13.5 M    Non-trainable params\n",
      "13.5 M    Total params\n",
      "54.197    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1431/1431 [01:33<00:00, 15.27it/s, v_num=0, train_loss_step=0.666, train_accuracy_step=0.600, train_precision_step=0.889, train_recall_step=0.615, train_loss_epoch=0.689, train_accuracy_epoch=0.546, train_precision_epoch=0.535, train_recall_epoch=0.796]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1431/1431 [01:35<00:00, 15.01it/s, v_num=0, train_loss_step=0.664, train_accuracy_step=0.533, train_precision_step=0.750, train_recall_step=0.333, train_loss_epoch=0.653, train_accuracy_epoch=0.672, train_precision_epoch=0.724, train_recall_epoch=0.510]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.057 >= min_delta = 0.0. New best score: 0.613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1431/1431 [01:35<00:00, 14.94it/s, v_num=0, train_loss_step=0.556, train_accuracy_step=0.733, train_precision_step=0.875, train_recall_step=0.700, train_loss_epoch=0.604, train_accuracy_epoch=0.695, train_precision_epoch=0.731, train_recall_epoch=0.569]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.042 >= min_delta = 0.0. New best score: 0.571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1431/1431 [01:35<00:00, 14.95it/s, v_num=0, train_loss_step=0.647, train_accuracy_step=0.600, train_precision_step=0.750, train_recall_step=0.375, train_loss_epoch=0.577, train_accuracy_epoch=0.712, train_precision_epoch=0.731, train_recall_epoch=0.627]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.025 >= min_delta = 0.0. New best score: 0.546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1431/1431 [01:36<00:00, 14.89it/s, v_num=0, train_loss_step=0.435, train_accuracy_step=0.733, train_precision_step=1.000, train_recall_step=0.500, train_loss_epoch=0.554, train_accuracy_epoch=0.729, train_precision_epoch=0.737, train_recall_epoch=0.671]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.029 >= min_delta = 0.0. New best score: 0.518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1431/1431 [01:35<00:00, 14.91it/s, v_num=0, train_loss_step=0.472, train_accuracy_step=0.867, train_precision_step=0.778, train_recall_step=1.000, train_loss_epoch=0.534, train_accuracy_epoch=0.741, train_precision_epoch=0.737, train_recall_epoch=0.708]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.023 >= min_delta = 0.0. New best score: 0.494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  99%|█████████▉| 1414/1431 [01:24<00:01, 16.70it/s, v_num=0, train_loss_step=0.452, train_accuracy_step=0.750, train_precision_step=0.750, train_recall_step=0.500, train_loss_epoch=0.534, train_accuracy_epoch=0.741, train_precision_epoch=0.737, train_recall_epoch=0.708]unfrozen\n",
      "Epoch 6: 100%|██████████| 1431/1431 [01:36<00:00, 14.81it/s, v_num=0, train_loss_step=0.721, train_accuracy_step=0.667, train_precision_step=0.625, train_recall_step=0.714, train_loss_epoch=0.521, train_accuracy_epoch=0.747, train_precision_epoch=0.736, train_recall_epoch=0.725]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.045 >= min_delta = 0.0. New best score: 0.450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1431/1431 [03:20<00:00,  7.13it/s, v_num=0, train_loss_step=0.268, train_accuracy_step=0.867, train_precision_step=0.750, train_recall_step=0.750, train_loss_epoch=0.311, train_accuracy_epoch=0.866, train_precision_epoch=0.857, train_recall_epoch=0.866] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.201 >= min_delta = 0.0. New best score: 0.249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1431/1431 [03:21<00:00,  7.11it/s, v_num=0, train_loss_step=0.273, train_accuracy_step=0.867, train_precision_step=0.750, train_recall_step=1.000, train_loss_epoch=0.203, train_accuracy_epoch=0.917, train_precision_epoch=0.913, train_recall_epoch=0.916]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.039 >= min_delta = 0.0. New best score: 0.210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 1431/1431 [03:21<00:00,  7.09it/s, v_num=0, train_loss_step=0.00204, train_accuracy_step=1.000, train_precision_step=1.000, train_recall_step=1.000, train_loss_epoch=0.0439, train_accuracy_epoch=0.984, train_precision_epoch=0.984, train_recall_epoch=0.984] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 0.210. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 1431/1431 [03:22<00:00,  7.07it/s, v_num=0, train_loss_step=0.00204, train_accuracy_step=1.000, train_precision_step=1.000, train_recall_step=1.000, train_loss_epoch=0.0439, train_accuracy_epoch=0.984, train_precision_epoch=0.984, train_recall_epoch=0.984]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "trainer.save_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c9359b9268a21020\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c9359b9268a21020\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=tb_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(version_prefix = \"sarcasm_detection_finetune_ckpt_v\"):\n",
    "    checkpoints = [file for file in os.listdir() if file.startswith(version_prefix) and file.endswith(\".ckpt\")]\n",
    "    return max(checkpoints, key=os.path.getctime) if checkpoints else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.trainer.Trainer to lightning.pytorch.trainer.trainer.Trainer\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.states.TrainerState to lightning.pytorch.trainer.states.TrainerState\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.states.TrainerStatus to lightning.pytorch.trainer.states.TrainerStatus\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.states.TrainerFn to lightning.pytorch.trainer.states.TrainerFn\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.connectors.data_connector._DataConnector to lightning.pytorch.trainer.connectors.data_connector._DataConnector\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.connectors.data_connector._DataHookSelector to lightning.pytorch.trainer.connectors.data_connector._DataHookSelector\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.utilities.parsing.AttributeDict to lightning.pytorch.utilities.parsing.AttributeDict\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.connectors.accelerator_connector._AcceleratorConnector to lightning.pytorch.trainer.connectors.accelerator_connector._AcceleratorConnector\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.strategies.single_device.SingleDeviceStrategy to lightning.pytorch.strategies.single_device.SingleDeviceStrategy\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.accelerators.cuda.CUDAAccelerator to lightning.pytorch.accelerators.cuda.CUDAAccelerator\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.plugins.precision.precision_plugin.PrecisionPlugin to lightning.pytorch.plugins.precision.precision_plugin.PrecisionPlugin\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.utilities.types.LRSchedulerConfig to lightning.pytorch.utilities.types.LRSchedulerConfig\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.connectors.logger_connector.logger_connector._LoggerConnector to lightning.pytorch.trainer.connectors.logger_connector.logger_connector._LoggerConnector\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.connectors.callback_connector._CallbackConnector to lightning.pytorch.trainer.connectors.callback_connector._CallbackConnector\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.connectors.checkpoint_connector._CheckpointConnector to lightning.pytorch.trainer.connectors.checkpoint_connector._CheckpointConnector\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.connectors.signal_connector._SignalConnector to lightning.pytorch.trainer.connectors.signal_connector._SignalConnector\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.fit_loop._FitLoop to lightning.pytorch.loops.fit_loop._FitLoop\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.training_epoch_loop._TrainingEpochLoop to lightning.pytorch.loops.training_epoch_loop._TrainingEpochLoop\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.progress._BatchProgress to lightning.pytorch.loops.progress._BatchProgress\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.progress._ProcessedTracker to lightning.pytorch.loops.progress._ProcessedTracker\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.progress._SchedulerProgress to lightning.pytorch.loops.progress._SchedulerProgress\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.progress._ReadyCompletedTracker to lightning.pytorch.loops.progress._ReadyCompletedTracker\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.optimization.automatic._AutomaticOptimization to lightning.pytorch.loops.optimization.automatic._AutomaticOptimization\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.progress._OptimizationProgress to lightning.pytorch.loops.progress._OptimizationProgress\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.progress._OptimizerProgress to lightning.pytorch.loops.progress._OptimizerProgress\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.progress._Progress to lightning.pytorch.loops.progress._Progress\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.progress._StartedTracker to lightning.pytorch.loops.progress._StartedTracker\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.optimization.manual._ManualOptimization to lightning.pytorch.loops.optimization.manual._ManualOptimization\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.evaluation_loop._EvaluationLoop to lightning.pytorch.loops.evaluation_loop._EvaluationLoop\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.connectors.logger_connector.result._ResultCollection to lightning.pytorch.trainer.connectors.logger_connector.result._ResultCollection\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.connectors.logger_connector.result._ResultMetric to lightning.pytorch.trainer.connectors.logger_connector.result._ResultMetric\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.connectors.logger_connector.result._Metadata to lightning.pytorch.trainer.connectors.logger_connector.result._Metadata\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.connectors.logger_connector.result._Sync to lightning.pytorch.trainer.connectors.logger_connector.result._Sync\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.trainer.connectors.data_connector._DataLoaderSource to lightning.pytorch.trainer.connectors.data_connector._DataLoaderSource\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.utilities.combined_loader.CombinedLoader to lightning.pytorch.utilities.combined_loader.CombinedLoader\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loops.prediction_loop._PredictionLoop to lightning.pytorch.loops.prediction_loop._PredictionLoop\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.callbacks.lr_monitor.LearningRateMonitor to lightning.pytorch.callbacks.lr_monitor.LearningRateMonitor\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.callbacks.early_stopping.EarlyStopping to lightning.pytorch.callbacks.early_stopping.EarlyStopping\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.callbacks.progress.tqdm_progress.TQDMProgressBar to lightning.pytorch.callbacks.progress.tqdm_progress.TQDMProgressBar\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.callbacks.model_summary.ModelSummary to lightning.pytorch.callbacks.model_summary.ModelSummary\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint to lightning.pytorch.callbacks.model_checkpoint.ModelCheckpoint\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.profilers.base.PassThroughProfiler to lightning.pytorch.profilers.base.PassThroughProfiler\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "/home/vscode/.local/lib/python3.9/site-packages/lightning/pytorch/_graveyard/legacy_import_unpickler.py:23: UserWarning: Redirecting import of pytorch_lightning.loggers.tensorboard.TensorBoardLogger to lightning.pytorch.loggers.tensorboard.TensorBoardLogger\n",
      "  warnings.warn(f\"Redirecting import of {module}.{name} to {new_module}.{name}\")\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ElectraClassifier' object has no attribute 'electra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     transfer_model \u001b[39m=\u001b[39m ElectraClassifier(data_module\u001b[39m=\u001b[39mdata_module, num_labels\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m, batch_size\u001b[39m=\u001b[39mdata_module\u001b[39m.\u001b[39mbatch_size)\n\u001b[1;32m     11\u001b[0m     \u001b[39m#load fine_tuned model weights keeping new classification head\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     transfer_model\u001b[39m.\u001b[39;49melectra\u001b[39m.\u001b[39mload_state_dict(loaded_model\u001b[39m.\u001b[39melectra\u001b[39m.\u001b[39mstate_dict())\n\u001b[1;32m     14\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNo model checkpoint found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ElectraClassifier' object has no attribute 'electra'"
     ]
    }
   ],
   "source": [
    "latest_checkpoint = find_latest_checkpoint()\n",
    "\n",
    "if latest_checkpoint:\n",
    "\n",
    "    #load model\n",
    "    loaded_model = ElectraClassifier.load_from_checkpoint(latest_checkpoint)\n",
    "\n",
    "    #new model with different classification heads\n",
    "    transfer_model = ElectraClassifier(data_module=data_module, num_labels=6, batch_size=data_module.batch_size)\n",
    "\n",
    "    #load fine_tuned model weights keeping new classification head\n",
    "    transfer_model.electra.load_state_dict(loaded_model.electra.state_dict())\n",
    "\n",
    "else:\n",
    "    print(\"No model checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 0it [03:14, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ElectraClassifier' object has no attribute 'predictions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#testing\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m test_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtest(model, data_module)\n\u001b[1;32m      4\u001b[0m test_preds \u001b[39m=\u001b[39m test_result[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mpreds\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m test_labels \u001b[39m=\u001b[39m test_result[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:706\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    704\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    705\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 706\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    707\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[1;32m    708\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:749\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, test_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[1;32m    746\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    747\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    748\u001b[0m )\n\u001b[0;32m--> 749\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    750\u001b[0m \u001b[39m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    751\u001b[0m results \u001b[39m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:971\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mbarrier(\u001b[39m\"\u001b[39m\u001b[39mrun-stage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    970\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m    973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[39m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py:375\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    374\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 375\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    379\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 288\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    290\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    291\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/strategies/strategy.py:387\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtest_step_context():\n\u001b[1;32m    386\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TestStep)\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtest_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[7], line 103\u001b[0m, in \u001b[0;36mElectraClassifier.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m f1_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf1(preds, labels)\n\u001b[1;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m'\u001b[39m\u001b[39mtest_f1\u001b[39m\u001b[39m'\u001b[39m, f1_score, on_step\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, on_epoch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictions\u001b[39m.\u001b[39mappend(preds)\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets\u001b[39m.\u001b[39mappend(labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ElectraClassifier' object has no attribute 'predictions'"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "\n",
    "test_result = trainer.test(model, data_module)\n",
    "test_preds = test_result[0]['preds']\n",
    "test_labels = test_result[0]['labels']\n",
    "#print(f\"test f1 score: {test_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SarcasmDataModule' object has no attribute 'collate_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predict_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mpredict(model, data_module)\n\u001b[1;32m      2\u001b[0m predict_preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mtensor(x[\u001b[39m'\u001b[39m\u001b[39mpreds\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m predict_result])\n\u001b[1;32m      3\u001b[0m predict_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mtensor(x[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m predict_result)])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:805\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    803\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    804\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 805\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    806\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\n\u001b[1;32m    807\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:847\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, predict_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[1;32m    844\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    845\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    849\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    850\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:973\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_loop\u001b[39m.\u001b[39mrun()\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m--> 973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    974\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m    975\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/loops/prediction_loop.py:101\u001b[0m, in \u001b[0;36m_PredictionLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m@_no_grad_context\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[_PREDICT_OUTPUT]:\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_data()\n\u001b[1;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip:\n\u001b[1;32m    103\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/loops/prediction_loop.py:127\u001b[0m, in \u001b[0;36m_PredictionLoop.setup_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    126\u001b[0m source \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_source\n\u001b[0;32m--> 127\u001b[0m dataloaders \u001b[39m=\u001b[39m _request_dataloader(source)\n\u001b[1;32m    128\u001b[0m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mbarrier(\u001b[39m\"\u001b[39m\u001b[39mpredict_dataloader()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dataloaders, CombinedLoader):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:328\u001b[0m, in \u001b[0;36m_request_dataloader\u001b[0;34m(data_source)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \n\u001b[1;32m    320\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39m    The requested dataloader\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[39mwith\u001b[39;00m _replace_dunder_methods(DataLoader, \u001b[39m\"\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m\"\u001b[39m), _replace_dunder_methods(BatchSampler):\n\u001b[1;32m    324\u001b[0m     \u001b[39m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001b[39;00m\n\u001b[1;32m    325\u001b[0m     \u001b[39m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[39m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[39m# methods so that the re-instantiated object is as close to the original as possible.\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m data_source\u001b[39m.\u001b[39;49mdataloader()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:298\u001b[0m, in \u001b[0;36m_DataLoaderSource.dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance, pl\u001b[39m.\u001b[39mLightningDataModule):\n\u001b[1;32m    297\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance\u001b[39m.\u001b[39mtrainer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_lightning_datamodule_hook(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minstance\u001b[39m.\u001b[39;49mtrainer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m    299\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:162\u001b[0m, in \u001b[0;36m_call_lightning_datamodule_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(fn):\n\u001b[1;32m    161\u001b[0m     \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningDataModule]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mdatamodule\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 162\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[4], line 83\u001b[0m, in \u001b[0;36mSarcasmDataModule.predict_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_dataloader\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mreturn\u001b[39;00m DataLoader(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, num_workers\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, collate_fn\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SarcasmDataModule' object has no attribute 'collate_fn'"
     ]
    }
   ],
   "source": [
    "predict_result = trainer.predict(model, data_module)\n",
    "predict_preds = torch.cat([torch.tensor(x['preds']) for x in predict_result])\n",
    "predict_labels = torch.cat([torch.tensor(x['labels'] for x in predict_result)])\n",
    "predict_f1_score = model.f1(predict_preds, predict_labels).item()\n",
    "print(f\"predict f1 score: {predict_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find most recent version of model checkpoint\n",
    "\n",
    "def find_latest_checkpoint(version_prefix = \"sarcasm_detection_finetune_ckpt_v\"):\n",
    "    checkpoints = [file for file in os.listdir() if file.startswith(version_prefix) and file.endswith(\".ckpt\")]\n",
    "    return max(checkpoints, key=os.path.getctime) if checkpoints else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ElectraClassifier' object has no attribute 'electra'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     transfer_model \u001b[39m=\u001b[39m ElectraClassifier(num_labels\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[39m#load fine_tuned model weights keeping new classification head\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     transfer_model\u001b[39m.\u001b[39;49melectra\u001b[39m.\u001b[39mload_state_dict(loaded_model\u001b[39m.\u001b[39melectra\u001b[39m.\u001b[39mstate_dict())\n\u001b[1;32m     14\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNo model checkpoint found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ElectraClassifier' object has no attribute 'electra'"
     ]
    }
   ],
   "source": [
    "latest_checkpoint = find_latest_checkpoint()\n",
    "\n",
    "if latest_checkpoint:\n",
    "\n",
    "    #load model\n",
    "    loaded_model = ElectraClassifier.load_from_checkpoint(latest_checkpoint)\n",
    "\n",
    "    #new model with different classification heads\n",
    "    transfer_model = ElectraClassifier(num_labels=6)\n",
    "\n",
    "    #load fine_tuned model weights keeping new classification head\n",
    "    transfer_model.electra.load_state_dict(loaded_model.electra.state_dict())\n",
    "\n",
    "else:\n",
    "    print(\"No model checkpoint found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraClassifier(\n",
       "  (model): ElectraForSequenceClassification(\n",
       "    (electra): ElectraModel(\n",
       "      (embeddings): ElectraEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 128)\n",
       "        (token_type_embeddings): Embedding(2, 128)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (encoder): ElectraEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): ElectraClassificationHead(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=256, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (train_accuracy): BinaryAccuracy()\n",
       "  (val_accuracy): BinaryAccuracy()\n",
       "  (train_precision): BinaryPrecision()\n",
       "  (val_precision): BinaryPrecision()\n",
       "  (train_recall): BinaryRecall()\n",
       "  (val_recall): BinaryRecall()\n",
       "  (val_f1_score): BinaryF1Score()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with a different number of classes\n",
    "transfer_model = ElectraClassifier(num_labels=3)\n",
    "\n",
    "# Load the fine-tuned model weights, but keep the new classification head\n",
    "transfer_model.model.electra.load_state_dict(loaded_model.model.electra.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\", \"accuracy\", \"precision\")\n",
    "results = metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"accuracy\", \"precision\", \"f1\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions = predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "\n",
    "predictions = trainer.predict(val_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\", \"accuracy\", \"precision\")\n",
    "results = metric.compute(predictions=preds, references=predictions.label_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_encodings = tokenizer.batch_encode_plus(list(train_df['headline']), max_length=512, padding='max_length', truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='pt')\n",
    "# val_encodings = tokenizer.batch_encode_plus(list(val_df['headline']), max_length=512, padding='max_length', truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='pt')\n",
    "# test_encodings = tokenizer.batch_encode_plus(list(test_df['headline']), max_length=512, padding=True, truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='pt')\n",
    "\n",
    "# train_input_ids = torch.tensor(train_encodings['input_ids'])\n",
    "# train_attention_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "# val_input_ids = torch.tensor(val_encodings['input_ids'])\n",
    "# val_attention_masks = torch.tensor(val_encodings['attention_mask'])\n",
    "# test_input_ids = torch.tensor(test_encodings['input_ids'])\n",
    "# test_attention_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "\n",
    "# train_labels = torch.tensor(train_df['is_sarcastic'].values)\n",
    "# val_labels = torch.tensor(val_df['is_sarcastic'].values)\n",
    "# test_labels = torch.tensor(test_df['is_sarcastic'].values)\n",
    "     \n",
    "        # steps_per_epoch = len(self.trainer.datamodule.train_dataloader()) // self.hparams.batch_size\n",
    "        # T_0 = max(steps_per_epoch // 2, 1)\n",
    "        # custom_scheduler = CustomScheduler(optimizer, self.warmup_steps, T_0)\n",
    "\n",
    "        # return {\n",
    "        # 'optimizer': optimizer,\n",
    "        # 'lr_scheduler': {\n",
    "        #     'scheduler': custom_scheduler,\n",
    "        #     'interval': 'step',\n",
    "        #     'frequency': 1\n",
    "        #     }\n",
    "        # }\n",
    "\n",
    "        \n",
    "        # warmup_steps = 10000\n",
    "        # total_steps = num_epochs * steps_per_epoch\n",
    "        # num_epochs = self.trainer.max_epochs\n",
    "\n",
    "        # if self.trainer.state.stage == 'fit':\n",
    "        #     steps_per_epoch = len(self.train_dataloader()// self.hparams.batch_size)\n",
    "        # else:\n",
    "        #     steps_per_epoch = 1\n",
    "\n",
    "        # warmup_steps = 10000\n",
    "        # total_steps = num_epochs * steps_per_epoch\n",
    "\n",
    "        # if total_steps <= warmup_steps:\n",
    "        #     total_steps = warmup_steps+1\n",
    "\n",
    "        # warmup_scheduler = get_cosine_schedule_with_warmup(\n",
    "        #     optimizer,\n",
    "        #     num_warmup_steps=warmup_steps,\n",
    "        #     num_training_steps=total_steps,\n",
    "        # )\n",
    "\n",
    "        # T_0 = max(steps_per_epoch // 2, 1)\n",
    "        # T_mult = 1\n",
    "        # adaptive_scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n",
    "\n",
    "        # def combined_scheduler(step):\n",
    "        #     if step < warmup_steps:\n",
    "        #         return warmup_scheduler.get_lr()[0]\n",
    "        #     else: \n",
    "        #         return adaptive_scheduler.get_lr()[0]\n",
    "\n",
    "        # scheduler = LambdaLR(optimizer, combined_scheduler)\n",
    "\n",
    "        # scheduler_config = {\n",
    "        #     'scheduler':scheduler,\n",
    "        #     'name':'learning_rate',\n",
    "        #     'interval':'step',\n",
    "        #     'frequency':1\n",
    "        # }\n",
    "\n",
    "        # # def lr_lambda(step):\n",
    "        # #     if step < 10000:\n",
    "        # #         return 0.00002\n",
    "        # #     elif step < 20000:\n",
    "        # #         return 0.0002\n",
    "        # #     else:\n",
    "        # #         return 0.002\n",
    "            \n",
    "        \n",
    "        # # lr_scheduler = {\n",
    "        # #     'scheduler':LambdaLR(optimizer, lr_lambda),\n",
    "        # #     'name': 'learning_rate',\n",
    "        # #     'interval':'step',\n",
    "        # #     'frequency': 1,\n",
    "\n",
    "        # # }\n",
    "\n",
    "        # print('initial learning rate:', optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "        # return [optimizer], [scheduler]\n",
    "\n",
    "    #     optimizer = AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "    #     scheduler_config = {\n",
    "    #     'scheduler': FinetuningScheduler(\n",
    "    #         optimizer,\n",
    "    #         milestones=[\n",
    "    #             (0, 0.00002),\n",
    "    #             (10000, 0.0002),\n",
    "    #             (20000, 0.002)\n",
    "    #         ],\n",
    "    #     ),\n",
    "    #     'name': 'learning_rate',\n",
    "    #     'interval': 'step',\n",
    "    #     'frequency': 1\n",
    "    # }\n",
    "    #     return [optimizer],[scheduler_config]\n",
    "\n",
    "# class CustomScheduler:\n",
    "#     def __init__(self, optimizer, warmup_steps, T_0, T_mult = 1):\n",
    "#         self.optimizer = self.optimizer\n",
    "#         self.warmup_steps = warmup_steps\n",
    "#         self.scheduler = CosineAnnealingLR(optimizer, T_0, T_mult)\n",
    "\n",
    "\n",
    "#     def step(self, global_step):\n",
    "#         if global_step > self.warmup_steps:\n",
    "#             self.scheduler.step(global_step - self.warmup_steps)\n",
    "\n",
    "#             for i, param_group in enumerate(self.optimizer.param_groups):\n",
    "#                 param_group['lr'] = self.scheduler.get_last_lr()[i]\n",
    "\n",
    "# class CustomScheduler:\n",
    "#     def __init__(self, optimizer, warmup_steps, T_0, T_mult=1):\n",
    "#         self.optimizer = optimizer\n",
    "#         self.warmup_steps = warmup_steps\n",
    "#         self.scheduler = CosineAnnealingLR(optimizer, T_0, T_mult)\n",
    "\n",
    "#     def step(self, global_step):\n",
    "#         if global_step > self.warmup_steps:\n",
    "#             self.scheduler.step(global_step - self.warmup_steps)\n",
    "\n",
    "#             for i, param_group in enumerate(self.optimizer.param_groups):\n",
    "#                 param_group['lr'] = self.scheduler.get_last_lr()[i]\n",
    "    \n",
    "#     def state_dict(self):\n",
    "#         return self.scheduler.state_dict()\n",
    "\n",
    "#     def load_state_dict(self, state_dict):\n",
    "#         self.scheduler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "    \n",
    "    # def on_train_batch_start(self, batch, batch_idx):\n",
    "\n",
    "    #     if self.global_step < self.warmup_steps:\n",
    "    #         for param in self.model.electra.parameters():\n",
    "    #             param.requires_grad = False\n",
    "\n",
    "    #     if self.global_step == self.warmup_steps:\n",
    "    #         #unfreeze base layers\n",
    "    #         for param in self.model.electra.parameters():\n",
    "    #             param.requires_grad = True\n",
    "\n",
    "    #         steps_per_epoch = len(self.trainer.datamodule.train_dataloader()) // self.hparams.batch_size\n",
    "    #         #T_0 = max(steps_per_epoch // 2, 1)\n",
    "\n",
    "    #         #self.custom_scheduler = CustomScheduler(self.optimizers, self.warmup_steps, T_0)\n",
    "\n",
    "    #     # if self.global_step > self.warmup_steps:\n",
    "    #     #     self.custom_scheduler.step(self.global_step)\n",
    "        \n",
    "    #     # current_lr = self.optimizers().param_groups[0]['lr']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
