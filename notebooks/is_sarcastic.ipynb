{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFElectraModel, ElectraTokenizer, TFElectraForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, GlobalMaxPooling1D\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "from keras.metrics import Precision, Recall\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import re \n",
    "import string\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "df_train = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines = True)\n",
    "\n",
    "df_train = df_train.drop(columns = ['article_link'])\n",
    "col_types = {'headline':'str', 'is_sarcastic':'int32'}\n",
    "df_train = df_train.astype(col_types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove stop words\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "df_train['headline'] = df_train['headline'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes of the training and validation sets\n",
    "train_size = int(0.8 * len(df_train))\n",
    "val_size = int(0.1 * len(df_train))\n",
    "test_size = len(df_train) - train_size - val_size\n",
    "\n",
    "# Split the DataFrame into training and validation sets\n",
    "train_df = df_train[:train_size]\n",
    "val_df = df_train[train_size:train_size + val_size]\n",
    "test_df = df_train[train_size + val_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 3.04MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 23.7kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 526kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 14:24:30.964302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 14:24:31.741106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 14:24:31.741165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 14:24:31.744266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 14:24:31.744423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 14:24:31.744484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 14:24:33.461691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 14:24:33.461933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 14:24:33.461949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-07 14:24:33.462008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-07 14:24:33.462051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8859 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-04-07 14:24:33.464773: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46888960 exceeds 10% of free system memory.\n",
      "2023-04-07 14:24:35.918938: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46888960 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer.batch_encode_plus(list(train_df['headline']), max_length=512, padding='max_length', truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='tf')\n",
    "val_encodings = tokenizer.batch_encode_plus(list(val_df['headline']), max_length=512, padding='max_length', truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='tf')\n",
    "test_encodings = tokenizer.batch_encode_plus(list(test_df['headline']), max_length=512, padding=True, truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='tf')\n",
    "\n",
    "train_input_ids = train_encodings['input_ids']\n",
    "train_attention_masks = train_encodings['attention_mask']\n",
    "val_input_ids = val_encodings['input_ids']\n",
    "val_attention_masks = val_encodings['attention_mask']\n",
    "test_input_ids = test_encodings['input_ids']\n",
    "test_attention_masks = test_encodings['attention_mask']\n",
    "\n",
    "train_labels = tf.constant(train_df['is_sarcastic'].values)\n",
    "val_labels = tf.constant(val_df['is_sarcastic'].values)\n",
    "test_labels = tf.constant(test_df['is_sarcastic'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tf_model.h5: 100%|██████████| 54.5M/54.5M [00:03<00:00, 15.4MB/s]\n",
      "2023-04-07 14:24:56.919433: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 15627264 exceeds 10% of free system memory.\n",
      "Some layers from the model checkpoint at google/electra-small-discriminator were not used when initializing TFElectraModel: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFElectraModel were initialized from the model checkpoint at google/electra-small-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "electra_model = TFElectraModel.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look into batch size effect on training\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels)).shuffle(100).batch(16)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels)).batch(16)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels)).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_learning_rate(epoch):\n",
    "  if epoch < 1000:\n",
    "    lr = (1e-3 - 1e-5) * epoch/1000 + 1e-5\n",
    "  else:\n",
    "      lr = 1e-3\n",
    "  if epoch == 1000:\n",
    "    n - 5\n",
    "    for layer in model.layers[:n]:\n",
    "      layer.trainable = True\n",
    "  return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(warmup_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 666/666 [00:00<00:00, 477kB/s]\n",
      "Downloading tf_model.h5: 100%|██████████| 438M/438M [00:27<00:00, 16.2MB/s] \n",
      "2023-04-07 14:26:48.980324: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "Some layers from the model checkpoint at google/electra-base-discriminator were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_electra_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " electra (TFElectraMainLayer  multiple                 108891648 \n",
      " )                                                               \n",
      "                                                                 \n",
      " classifier (TFElectraClassi  multiple                 592130    \n",
      " ficationHead)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 592,130\n",
      "Non-trainable params: 108,891,648\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "\n",
    "# Load the Electra model pre-trained on a large corpus of text\n",
    "model = TFElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator')\n",
    "#model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "for layer in model.layers[:-1]:\n",
    "  layer.trainable = False\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Define the input layers for your model\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# Pass the inputs through the Electra model\n",
    "outputs = model({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "\n",
    "# Replace the classifier layer with a new layer for your specific task\n",
    "classifier = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(outputs[0])\n",
    "\n",
    "# Define the input and output layers of the model\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=classifier)\n",
    "\n",
    "# Compile the model with an appropriate loss function and optimizer\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1431/1431 [==============================] - ETA: 0s - loss: 0.6692 - accuracy: 0.6107 - precision: 0.6256 - recall: 0.4586"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 14:37:43.644771: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [2861]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1431/1431 [==============================] - 549s 384ms/step - loss: 0.6692 - accuracy: 0.6107 - precision: 0.6256 - recall: 0.4586 - val_loss: 0.6487 - val_accuracy: 0.6459 - val_precision: 0.6373 - val_recall: 0.5831 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff56d1b3cd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on your dataset\n",
    "model.fit(train_dataset, epochs=1, callbacks = [lr_scheduler, es], validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Electra_sd_finetune_1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot assign value to variable ' tf_electra_for_sequence_classification_1/electra/embeddings/word_embeddings/weight:0': Shape mismatch.The variable shape (30522, 768), and the assigned value shape (768, 768) are incompatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loaded_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39mElectra_sd_finetune_1.keras\u001b[39;49m\u001b[39m'\u001b[39;49m, custom_objects \u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mTFElectraForSequenceClassification\u001b[39;49m\u001b[39m\"\u001b[39;49m: TFElectraForSequenceClassification})\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/saving/saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[1;32m    205\u001b[0m         filepath,\n\u001b[1;32m    206\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[1;32m    207\u001b[0m         \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m,\n\u001b[1;32m    208\u001b[0m         safe_mode\u001b[39m=\u001b[39msafe_mode,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m    213\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39;49mcustom_objects, \u001b[39mcompile\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcompile\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    214\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/backend.py:4360\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[0;34m(variable, value)\u001b[0m\n\u001b[1;32m   4357\u001b[0m     variable\u001b[39m.\u001b[39massign(d_value)\n\u001b[1;32m   4358\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4359\u001b[0m     \u001b[39m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[0;32m-> 4360\u001b[0m     variable\u001b[39m.\u001b[39;49massign(value)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot assign value to variable ' tf_electra_for_sequence_classification_1/electra/embeddings/word_embeddings/weight:0': Shape mismatch.The variable shape (30522, 768), and the assigned value shape (768, 768) are incompatible."
     ]
    }
   ],
   "source": [
    "loaded_model = tf.keras.models.load_model('Electra_sd_finetune_1.keras', custom_objects ={\"TFElectraForSequenceClassification\": TFElectraForSequenceClassification})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
