{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFElectraModel, ElectraTokenizer, TFElectraForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, GlobalMaxPooling1D\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "from keras.metrics import Precision, Recall\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import re \n",
    "import string\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines = True)\n",
    "df_train = df_train.drop(columns = ['article_link'])\n",
    "col_types = {'headline':'str', 'is_sarcastic':'int32'}\n",
    "df_train = df_train.astype(col_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    # Remove stop words\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sizes of the training and validation sets\n",
    "train_size = int(0.8 * len(df_train))\n",
    "val_size = int(0.1 * len(df_train))\n",
    "test_size = len(df_train) - train_size - val_size\n",
    "\n",
    "# Split the DataFrame into training and validation sets\n",
    "train_df = df_train[:train_size]\n",
    "val_df = df_train[train_size:train_size + val_size]\n",
    "test_df = df_train[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer.batch_encode_plus(list(train_df['headline']), max_length=512, padding='max_length', truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='tf')\n",
    "val_encodings = tokenizer.batch_encode_plus(list(val_df['headline']), max_length=512, padding='max_length', truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='tf')\n",
    "test_encodings = tokenizer.batch_encode_plus(list(test_df['headline']), max_length=512, padding=True, truncation=True, return_attention_mask=True, return_token_type_ids=False, return_tensors='tf')\n",
    "\n",
    "train_input_ids = train_encodings['input_ids']\n",
    "train_attention_masks = train_encodings['attention_mask']\n",
    "val_input_ids = val_encodings['input_ids']\n",
    "val_attention_masks = val_encodings['attention_mask']\n",
    "test_input_ids = test_encodings['input_ids']\n",
    "test_attention_masks = test_encodings['attention_mask']\n",
    "\n",
    "train_labels = tf.constant(train_df['is_sarcastic'].values)\n",
    "val_labels = tf.constant(val_df['is_sarcastic'].values)\n",
    "test_labels = tf.constant(test_df['is_sarcastic'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels)).shuffle(100).batch(16)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels)).batch(16)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels)).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_learning_rate(epoch):\n",
    "  if epoch < 1000:\n",
    "    lr = (1e-3 - 1e-5) * epoch/1000 + 1e-5\n",
    "  else:\n",
    "      lr = 1e-3\n",
    "  if epoch == 1000:\n",
    "    n - 5\n",
    "    for layer in model.layers[:n]:\n",
    "      layer.trainable = True\n",
    "  return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(warmup_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 10)\n",
    "\n",
    "# Load the Electra model pre-trained on a large corpus of text\n",
    "model = TFElectraForSequenceClassification.from_pretrained('google/electra-base-discriminator')\n",
    "#model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "for layer in model.layers[:-1]:\n",
    "  layer.trainable = False\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Define the input layers for your model\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# Pass the inputs through the Electra model\n",
    "outputs = model({'input_ids': input_ids, 'attention_mask': attention_mask})\n",
    "\n",
    "# Replace the classifier layer with a new layer for your specific task\n",
    "classifier = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(outputs[0])\n",
    "\n",
    "# Define the input and output layers of the model\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=classifier)\n",
    "\n",
    "# Compile the model with an appropriate loss function and optimizer\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on your dataset\n",
    "model.fit(train_dataset, epochs=10000, callbacks = [lr_scheduler, es], validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Electra_sarcasm_detection_finetune.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_dataset)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_preds = model.predict(test_dataset)\n",
    "binary_test_preds = (test_preds > 0.5).astype(int)\n",
    "f1 = f1_score(y_test, binary_test_preds, average='macro')\n",
    "f1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
